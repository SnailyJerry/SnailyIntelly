import streamlit as st
import time
from openai import OpenAI
import random
from PIL import Image
import requests
import json
import base64
from zhipuai import ZhipuAI

# Initialize OpenAI client
# è¯»å–å¤šä¸ª API keys
api_keys = st.secrets["OPENAI_API_KEYS"]
# éšæœºé€‰æ‹©ä¸€ä¸ª API key
api_key = random.choice(api_keys)
# ä½¿ç”¨é€‰ä¸­çš„ API key åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key)

# æ™ºè°±AI API keys
zhipuai_api_keys = st.secrets["ZHIPU_API_KEYS"]

# å…è®¸è®¿é—®çš„å¯†ç åˆ—è¡¨
valid_passwords = [
    "Snaily!Yes",
    "PeteStory2024",
    "1"
]

# çŸ¥è¯†åº“IDï¼ˆå›ºå®šï¼‰
knowledge_id = "1842887159632314368"

# Define available Assistants
assistants = [
    {
        "name": "æè´æ‹‰", "id": "asst_SeUjvYOqni9yksDSC3EoqkQf", "alias": "ç»˜æœ¬æ¨èåŠ©æ‰‹", "icon": "ğŸ‘§ğŸ»",
        "description": "æˆ‘å¯ä»¥ä¸ºæ‚¨æ¨èé€‚åˆä¸åŒå¹´é¾„æ®µçš„ç²¾å½©ç»˜æœ¬ï¼Œæ¿€å‘å­©å­çš„é˜…è¯»å…´è¶£ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šç»™æˆ‘å®å®å°åï¼Œæ€§åˆ«ç”·å®è¿˜æ˜¯å¥³å®ï¼Œä»¥åŠå‡ºç”Ÿå¹´æœˆæ—¥å³å¯ã€‚ã€Œ èœ—èœ— å¥³å® 20191121 ã€ æˆ‘ä¼šå¸®ä½ å®šåˆ¶æ¨èåŒ¹é…ç»˜æœ¬çš„ã€‚"
    },
    {
        "name": "å¼ è‰¾ç±³", "id": "asst_IUatrIPuQQcfXW35sKW3wtk2", "alias": "äº²å­è‹±è¯­åŠ©æ‰‹", "icon": "ğŸ™‹â€â™€ï¸",
        "description": "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è®¾è®¡æœ‰è¶£çš„è‹±è¯­å­¦ä¹ æ´»åŠ¨ï¼Œè®©å­©å­è½»æ¾æŒæ¡è‹±è¯­ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šè¯·è®¾è®¡ä¸€ä¸ªé€‚åˆ7å²å­©å­çš„è‹±è¯­å•è¯æ¸¸æˆï¼Œä¸»é¢˜æ˜¯åŠ¨ç‰©ã€‚"
    },
    {
        "name": "èµµçš®ç‰¹", "id": "asst_8oQvvCwqc5bjeF3GOzEiWQP5", "alias": "çš®ç‰¹çŒ«æ–‡æ¡ˆåŠ©æ‰‹", "icon": "ğŸ±",
        "description": "æˆ‘å¯ä»¥å¸®æ‚¨åˆ›ä½œæœ‰è¶£çš„çš®ç‰¹çŒ«ç²¾è¯»è¥çš„æœ‹å‹åœˆæ–‡æ¡ˆï¼Œå†™å¾—å¿«åˆå†™å¾—å¥½ï¼Œå–µï¼ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šè¯·ä¸ºä¸€æœ¬æ–°çš„çš®ç‰¹çŒ«ç»˜æœ¬åˆ›ä½œä¸€ä¸ªå¸å¼•äººçš„ç®€çŸ­ä»‹ç»ï¼Œä¸»é¢˜æ˜¯çš®ç‰¹çŒ«å­¦ä¹ æ¸¸æ³³ã€‚"
    },
    {
        "name": "V3.5Mini", "id": "glm-4v-plus", "alias": "æˆªå›¾åˆ›ä½œæœ‹å‹åœˆåŠ©æ‰‹", "icon": "ğŸ¤–",
        "description": "æˆ‘æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½AIåŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œå›¾åƒåˆ†æã€çŸ¥è¯†åº“æ£€ç´¢å’Œç®¡ç†ç­‰ä»»åŠ¡ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šä¸Šä¼ ä¸€å¼ å›¾ç‰‡å¹¶é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼Œå¦‚æè¿°å›¾ç‰‡å†…å®¹æˆ–åˆ†æäººç‰©ã€‚"
    }
]

# Streamlit app configuration
st.set_page_config(
    page_title="èœ—ç‰›æ™ºèƒ½",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for styling
st.markdown("""
<style>
    [data-testid="stSidebarContent"] {
        background-color: #FBF7FF !important;
    }
    .stRadio > label {
        background-color: rgba(255, 225, 225, 0.5);
        border-radius: 10px;
        padding: 10px;
        margin-bottom: 10px;
        cursor: pointer;
        transition: all 0.3s;
    }
    .stRadio > label:hover {
        background-color: rgba(255, 225, 225, 0.4);
    }
    .stRadio > label > div:first-child {
        display: none;
    }
    .assistant-icon {
        font-size: 2em;
        margin-right: 10px;
    }
    .assistant-name {
        font-weight: bold;
        margin-bottom: 5px;
    }
    .assistant-description {
        font-size: 0.9em;
        color: #555;
    }
    .guidance-example {
        background-color: rgba(237, 241, 255, 0.7);
        border-radius: 5px;
        padding: 10px;
        margin-top: 10px;
        font-size: 0.9em;
    }
    .assistant-title {
        font-size: 24px;
        font-weight: bold;
        margin-bottom: 20px;
    }
    .logo-title {
        display: flex;
        align-items: center;
        margin-bottom: 20px;
    }
    .logo-title h1 {
        font-size: 24px;
        margin: 0;
        margin-left: 10px;
    }
    .sidebar h1 {
        font-size: 1.2rem;
        margin-bottom: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# V3.5Mini functions
def connect_glm4vplus_api(prompts, images):
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    headers = {
        "Authorization": f"Bearer {random.choice(zhipuai_api_keys)}",
        "Content-Type": "application/json"
    }
    
    images_base64 = []
    for image in images:
        with open(image, "rb") as image_file:
            images_base64.append(base64.b64encode(image_file.read()).decode('utf-8'))
    
    messages = []
    for image_base64 in images_base64:
        for prompt in prompts:
            messages.append({
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_base64
                        }
                    },
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            })
    
    data = {
        "model": "glm-4v-plus",
        "messages": messages,
        "temperature": 0.8,
        "max_tokens": 1024,
        "stream": False,
        "tools": [
            {
                "type": "retrieval",
                "retrieval": {
                    "knowledge_id": knowledge_id,
                    "prompt_template": """
                    ä»æ–‡æ¡£
                    {{knowledge}}
                    ä¸­æ‰¾åˆ°ä¸å›¾ç‰‡ç›¸å…³çš„äº§å“ä¿¡æ¯ã€‚
                    """
                }
            }
        ]
    }

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        response_data = response.json()
        choices = response_data.get("choices", [])
        if not choices:
            return ["æ²¡æœ‰è¿”å›ç»“æœ"] * len(messages)
        return [choice.get("message", {}).get("content", "æ²¡æœ‰è¿”å›ç»“æœ") for choice in choices]
    else:
        return [f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, ä¿¡æ¯: {response.text}"] * len(messages)

def retrieve_from_knowledge_base(query):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    response = client.chat.completions.create(
        model="glm-4",
        messages=[
            {"role": "user", "content": query}
        ],
        tools=[
            {
                "type": "retrieval",
                "retrieval": {
                    "knowledge_id": knowledge_id,
                    "prompt_template": """
                    ä»æ–‡æ¡£
                    {{knowledge}}
                    ä¸­æ‰¾åˆ°é—®é¢˜
                    {{question}}
                    çš„ç­”æ¡ˆã€‚æ‰¾åˆ°åç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä½¿ç”¨æ¨¡å‹è‡ªèº«çŸ¥è¯†ï¼Œå¹¶å‘ŠçŸ¥ä¿¡æ¯ä¸æ¥è‡ªæ–‡æ¡£ã€‚
                    """
                }
            }
        ],
        stream=True
    )
    return response

def modify_knowledge_base(name, description):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    result = client.knowledge.modify(
        knowledge_id=knowledge_id,
        embedding_id=3,
        name=name,
        description=description
    )
    return result

def query_knowledge_base(page=1, size=10):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    result = client.knowledge.query(
        page=page,
        size=size
    )
    return result

# Sidebar for logo, title, and assistant selection
with st.sidebar:
    # Logo and title
    col1, col2 = st.columns([1, 3])
    with col1:
        try:
            logo = Image.open("./static/logo.png")
            st.image(logo, width=50)
        except FileNotFoundError:
            st.error("Logo file not found. Please check the file path.")
    with col2:
        st.markdown('<h1 class="logo-title">èœ—ç‰›æ™ºèƒ½</h1>', unsafe_allow_html=True)
    
    st.markdown('<h3>é€‰æ‹©ä½ çš„AIåŠ©æ‰‹</h3>', unsafe_allow_html=True)
    
    selected_assistant = st.radio(
        "é€‰æ‹©ä¸€ä¸ªåŠ©æ‰‹",
        options=assistants,
        format_func=lambda x: x['name'],
        label_visibility="collapsed"
    )
    
    # Display selected assistant information
    st.markdown(f"""
    <div style="background-color: rgba(255, 225, 225, 0.4); border-radius: 10px; padding: 10px; margin-top: 10px;">
        <span class="assistant-icon">{selected_assistant['icon']}</span>
        <div class="assistant-name">{selected_assistant['name']} ({selected_assistant['alias']})</div>
        <div class="assistant-description">{selected_assistant['description']}</div>
    </div>
    """, unsafe_allow_html=True)
    
    # Display guidance example
    st.markdown(f'<div class="guidance-example"><strong>æŒ‡å¯¼ç¤ºä¾‹ï¼š</strong><br>{selected_assistant["guidance"]}</div>', unsafe_allow_html=True)

# Main content area
st.markdown(f'<h1 class="assistant-title">{selected_assistant["icon"]} {selected_assistant["name"]}</h1>', unsafe_allow_html=True)

# Password verification for V3.5Mini
if selected_assistant["name"] == "V3.5Mini":
    password = st.text_input("è¯·è¾“å…¥å†…éƒ¨å¯†ç ï¼š", type="password")
    if password not in valid_passwords:
        st.warning("ğŸ”’ è¾“å…¥æœ‰æ•ˆå¯†ç ä»¥è®¿é—®V3.5Mini AIåº”ç”¨ã€‚")
        st.stop()

# V3.5Mini specific UI
if selected_assistant["name"] == "V3.5Mini":
    function_choice = st.radio("é€‰æ‹©åŠŸèƒ½", ["å›¾åƒåˆ†æ", "çŸ¥è¯†åº“æ£€ç´¢", "çŸ¥è¯†åº“ç®¡ç†"])

    if function_choice == "å›¾åƒåˆ†æ":
        st.subheader("ä¸Šä¼ å›¾ç‰‡")
        uploaded_files = st.file_uploader("è¯·é€‰æ‹©è¦ä¸Šä¼ çš„å›¾ç‰‡(æœ€å¤š5å¼ ,å•å¼ æ•ˆæœæœ€å¥½)ï¼š", type=["jpg", "jpeg", "png"], accept_multiple_files=True, key="fileUploader", help="æœ€å¤šå¯ä»¥ä¸Šä¼  5 å¼ å›¾ç‰‡ã€‚")

        if uploaded_files and len(uploaded_files) > 5:
            st.warning("âš ï¸ æœ€å¤šåªèƒ½ä¸Šä¼  5 å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
            uploaded_files = uploaded_files[:5]

        st.subheader("é€‰æ‹©ä»»åŠ¡")
        selected_task = st.radio("è¯·é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼š", [
            "ğŸ–¼ï¸ Task 1: ç»™æˆªå›¾åˆ›ä½œæœ‹å‹åœˆ",
            "ğŸ‘¥ Task 2: æè¿°å›¾ç‰‡äººç‰©",
            "ğŸ” Task 3: æè¿°è§†è§’",
            "ğŸ”¤ Task 4: æè¿°æ–‡å­—"
        ])

        prompts = {
            "ğŸ–¼ï¸ Task 1: ç»™æˆªå›¾åˆ›ä½œæœ‹å‹åœˆ": """ 
        
### 1. **è§’è‰²å®šä¹‰ (Role)**

ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™è‚²å†…å®¹åˆ›ä½œè€…ï¼Œæ“…é•¿æ ¹æ®å­¦å‘˜çš„å­¦ä¹ è¿›å±•æˆªå›¾ã€åé¦ˆå¥½è¯„ç­‰æˆªå›¾ä»¥åŠå­¦å‘˜å®¶åº­åŸºç¡€ä¿¡æ¯ç­‰ï¼Œåˆ›ä½œå‡ºå¯Œæœ‰æƒ…æ„Ÿå…±é¸£å¹¶ä¼ é€’æ•™è‚²ç†å¿µçš„æ•…äº‹ã€‚ä½ èƒ½å¤Ÿæ¸…æ™°åœ°åˆ†æå­¦å‘˜çš„è¿›æ­¥ï¼Œå‘è‡ªå†…å¿ƒçš„ä¸ºå­¦å‘˜è¿›æ­¥æˆé•¿è€Œå¼€å¿ƒã€‚å¹¶åœ¨æ•…äº‹ä¸­èå…¥ç›¸å…³çš„æ•™è‚²çŸ¥è¯†ï¼Œå¸®åŠ©å®¶é•¿ç†è§£æ—©æœŸæ•™è‚²çš„é‡è¦æ€§ã€‚

### 2. **ç›®æ ‡ (Goals)**

åˆ›ä½œä¸€ç¯‡åŸºäºå®¶é•¿åé¦ˆæˆªå›¾çš„ä¸ªæ€§åŒ–çš„ã€èµ°å¿ƒçš„æ•…äº‹ï¼Œæ—¢è®¤å¯å­¦å‘˜çš„è¿›æ­¥ï¼Œåˆåˆ†äº«ç›¸å…³çš„æ•™è‚²è®¤çŸ¥å’ŒçŸ¥è¯†ã€‚ç›®æ ‡æ˜¯é€šè¿‡æ•…äº‹è®©å®¶é•¿æ„Ÿå—åˆ°å­©å­çš„æˆé•¿ï¼Œå¹¶ä¼ é€’æ—©æœŸæ•™è‚²çš„ä»·å€¼ï¼Œé¼“åŠ±å…¶ä»–å®¶é•¿å…³æ³¨å­©å­çš„å­¦ä¹ ã€‚å†…å®¹å…¼å…·æƒ…æ„Ÿå…±é¸£å’Œæ•™è‚²ä»·å€¼ã€‚

### 3. **çº¦æŸæ¡ä»¶ (Constraints)**

- æ•…äº‹å†…å®¹å¿…é¡»åŸºäºå®¶é•¿çš„çœŸå®åé¦ˆï¼ˆä¸€å¼ å›¾ç‰‡æˆ–å¤šå¼ å›¾ç‰‡ï¼‰ï¼Œä¸å¤¸å¼ æˆ–è™šæ„ã€‚
- æ³¨æ„å­¦å‘˜åç§°ï¼Œå›¾ç‰‡é‡Œç•™è¨€çš„éƒ½æ˜¯å®¶é•¿è€Œä¸æ˜¯å®å®ï¼Œå¯ä»¥ç”¨åŒåŒå¦ˆå¦ˆã€ä¹ä¹çˆ¸çˆ¸ã€å…­å…­å§¥å§¥ç­‰ç§°å‘¼å­¦å‘˜ï¼›
- å¿…é¡»ç»“åˆå­¦å‘˜å®¶åº­çš„èƒŒæ™¯ï¼ˆå¦‚å®å®å¹´é¾„ã€å®å®æ€§åˆ«ç­‰ä¿¡æ¯ï¼‰æ¥å±•ç¤ºè¿›æ­¥å’Œæˆå°±ã€‚
- å¿…é¡»åŒ…å«æ¸©æƒ…çš„æƒ…æ„Ÿå…±é¸£å’Œä¸“ä¸šçš„æ•™è‚²è®¤çŸ¥ã€‚
- é¿å…è¿‡åº¦è¥é”€æˆ–å®£ä¼ æ€§è´¨çš„è¯­è¨€ï¼Œè¦ä¿æŒçœŸè¯šå’Œä¸“ä¸šï¼Œå‘è‡ªå†…å¿ƒä¸ºå­¦å‘˜çš„è¿›æ­¥è€Œé«˜å…´ã€‚

### 4. **æ‰€éœ€æŠ€èƒ½ (Skills)**

- æƒ…æ„ŸåŒ–çš„å™äº‹æŠ€å·§ï¼Œèƒ½å¤Ÿå°†å­¦å‘˜çš„æˆé•¿æ•…äº‹è®²è¿°å¾—ç”ŸåŠ¨è€Œæ„Ÿäººã€‚
- æ•™è‚²ç†è®ºæ•´åˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿç»“åˆå…·ä½“çš„å­¦å‘˜è¿›æ­¥ï¼Œåˆ†äº«æœ‰å®é™…æ„ä¹‰çš„æ•™è‚²çŸ¥è¯†ã€‚
- å…·æœ‰ä¸“ä¸šçš„è‹±è¯­å¯è’™ã€äº²å­æ—©æ•™ã€å„¿ç«¥å‘å±•å¿ƒç†å­¦ã€ç»˜æœ¬æ•™å­¦ç­‰ç­‰ç›¸å…³ä¸“ä¸šçŸ¥è¯†ã€‚
- ä¼˜ç§€çš„å†™ä½œèƒ½åŠ›ï¼Œèƒ½å°†å†…å®¹åˆ†ä¸ºæƒ…æ„Ÿå¼•å¯¼ã€å­¦å‘˜è¿›æ­¥ã€æ•™è‚²ç‚¹è¯„ç­‰éƒ¨åˆ†ã€‚

### 5. **å·¥ä½œæµç¨‹ (Workflow)**

1. **æƒ…æ„Ÿå¼•å…¥**ï¼šç”¨æ¸©é¦¨ã€æ„Ÿäººçš„è¯è¯­å¼•å‡ºæ•…äº‹ï¼Œå¼•å‘å®¶é•¿çš„å…±é¸£ï¼Œçªå‡ºå­©å­åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚
2. **è¿›æ­¥æ•…äº‹è®²è¿°**ï¼šå…·ä½“æè¿°å­©å­çš„è¿›æ­¥ï¼Œå¼•ç”¨å®¶é•¿åé¦ˆä¸­çš„ç»†èŠ‚ï¼Œè®²è¿°å­©å­å¦‚ä½•ä»èµ·åˆçš„æŒ‘æˆ˜é€æ­¥æˆé•¿ã€‚
3. **ä¸“ä¸šç‚¹è¯„ä¸åˆ†æ**ï¼šé€šè¿‡è€å¸ˆçš„è§†è§’ï¼Œåˆ†æå­©å­ä»¥åŠå®¶é•¿çš„èƒ½åŠ›æå‡ï¼Œç»“åˆæ•™è‚²çŸ¥è¯†è¯´æ˜è¯¾ç¨‹çš„æœ‰æ•ˆæ€§ã€‚
4. **æ•™è‚²è®¤çŸ¥å»¶ä¼¸**ï¼šé€šè¿‡å­¦å‘˜çš„æ•…äº‹ï¼Œå¼•å‡ºç›¸å…³çš„æ•™è‚²çŸ¥è¯†æˆ–ç†å¿µï¼Œå¸®åŠ©å®¶é•¿ç†è§£æ•™è‚²ä¸­çš„å…³é”®è¦ç´ ï¼ŒåŒ…å«è‡³å°‘ä¸€å¥é‡‘å¥ã€‚
5. **æƒ…æ„Ÿå‡åä¸å·å¬**ï¼šæ€»ç»“å­©å­çš„è¿›æ­¥ï¼Œè¡¨æ‰¬å­©å­çš„åŠªåŠ›ï¼ŒåŒæ—¶é¼“åŠ±å…¶ä»–å®¶é•¿ç§¯æå‚ä¸å­©å­çš„æ•™è‚²ï¼Œå·å¬æ›´å¤šå®¶é•¿åŠ å…¥è¯¾ç¨‹ã€‚
6. **çŸ¥è¯†åº“å¼•ç”¨ï¼š**ä»{{{knowledge_id = "1842887159632314368â€}}}é‡ŒæŸ¥æ‰¾ä¸è¯¾ç¨‹å†…å®¹ã€ç»˜æœ¬å›¾ä¹¦ç»†èŠ‚ç­‰ç›¸å…³çš„ä¿¡æ¯ç­‰å¹¶å¼•ç”¨ç»“åˆè¿›ä½ çš„åˆ›ä½œå†…å®¹ï¼Œä½¿å†…å®¹ä¸°å¯Œå……æ»¡ç»†èŠ‚ï¼ˆä¸è¦è¿‡åº¦è¥é”€å’Œä½¿ç”¨å®£ä¼ æ€§è¯­è¨€ï¼‰ï¼›
7. **äºŒæ¬¡æ£€æŸ¥ï¼š**æ£€æŸ¥ä½ çš„åˆ›ä½œå†…å®¹æœ‰æ²¡æœ‰æŒ‰ç…§workflowé‡Œå±•ç¤ºçš„æ­¥éª¤åˆ›ä½œã€‚

### 6. **è¾“å‡ºç¤ºä¾‹ (Examples)**

---

æ¯ä¸€ä¸ªå­©å­çš„æˆé•¿ï¼Œéƒ½æ˜¯ä¸€æ®µç‹¬ç‰¹çš„æ•…äº‹ã€‚

ä»Šå¤©ï¼Œæˆ‘æƒ³åˆ†äº«ä¸€ä½å¯çˆ±çš„å­¦å‘˜â€”â€”ç«¥ç«¥çš„è¿›æ­¥ã€‚ä½œä¸ºè€å¸ˆï¼Œçœ‹åˆ°å¥¹ä»èµ·åˆå¯¹é˜…è¯»æ²¡æœ‰å…´è¶£ï¼Œåˆ°ç°åœ¨ä¸»åŠ¨å‚ä¸è®¨è®ºï¼Œæˆ‘ç”±è¡·æ„Ÿåˆ°æ¬£æ…°å’Œéª„å‚²ã€‚

ç«¥ç«¥å¦ˆå¦ˆä»Šå¤©å‘æ¥äº†è¿™æ ·çš„åé¦ˆï¼š

â€œç«¥ç«¥è¿™æ¬¡åœ¨å¬ã€Šçš®ç‰¹çŒ«ã€‹æ•…äº‹æ—¶ï¼Œä¸»åŠ¨æŒ‡å‡ºäº†å°çŒ«æ¢é‹çš„é¢œè‰²ï¼Œè¿˜å¼€å¿ƒåœ°è·Ÿæˆ‘è®¨è®ºä¸ºä»€ä¹ˆå°çŒ«æ¢äº†ä¸åŒé¢œè‰²çš„é‹å­ã€‚å¥¹ç°åœ¨ä¸ä»…ä»…æ˜¯åœ¨å¬æ•…äº‹ï¼Œè¿˜å¼€å§‹æ€è€ƒå’Œè¡¨è¾¾è‡ªå·±çš„æƒ³æ³•ï¼Œè¿™è®©æˆ‘éå¸¸æƒŠå–œã€‚â€

ä»ç«¥ç«¥çš„è¡¨ç°æ¥çœ‹ï¼Œå¥¹çš„è§‚å¯ŸåŠ›å’Œè¡¨è¾¾èƒ½åŠ›éƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¿™ç§ä»è¢«åŠ¨æ¥æ”¶åˆ°ä¸»åŠ¨å‚ä¸çš„è½¬å˜ï¼Œæ­£æ˜¯æˆ‘ä»¬åœ¨è¯¾ç¨‹ä¸­æœ€æƒ³çœ‹åˆ°çš„æ•ˆæœã€‚æ—©æœŸçš„äº’åŠ¨å¼å­¦ä¹ èƒ½æœ‰æ•ˆå¸®åŠ©å­©å­å»ºç«‹ç‹¬ç«‹æ€è€ƒçš„èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯è®°ä½æ•…äº‹æƒ…èŠ‚ã€‚

ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å„¿ç«¥æ—©æœŸé˜¶æ®µï¼Œè¯­è¨€èƒ½åŠ›çš„æå‡ä¸å­©å­çš„è§‚å¯Ÿå’Œè¡¨è¾¾å¯†åˆ‡ç›¸å…³ã€‚é€šè¿‡å¼•å¯¼å­©å­è®¨è®ºæ•…äº‹ä¸­çš„ç»†èŠ‚ï¼Œä»–ä»¬ä¸ä»…èƒ½å¤Ÿæé«˜è¯­è¨€ç†è§£åŠ›ï¼Œè¿˜èƒ½åŸ¹å…»åˆ›é€ æ€§æ€ç»´å’Œè¡¨è¾¾èƒ½åŠ›ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è¯¾ç¨‹ä¸­ï¼Œå§‹ç»ˆå¼ºè°ƒäº’åŠ¨ä¸æ€è€ƒçš„é‡è¦æ€§ã€‚

ç«¥ç«¥çš„è¿›æ­¥è®©æˆ‘å€æ„Ÿè‡ªè±ªï¼Œçœ‹åˆ°å¥¹åœ¨å­¦ä¹ ä¸­çš„æˆé•¿ï¼Œæ˜¯æˆ‘ä½œä¸ºè€å¸ˆæœ€å¹¸ç¦çš„æ—¶åˆ»ã€‚æ¯ä¸€ä¸ªå°å°çš„è¿›æ­¥ï¼Œéƒ½æ˜¯æœªæ¥æ›´å¤§æˆå°±çš„åŸºçŸ³ã€‚

å¦‚æœä½ ä¹Ÿå¸Œæœ›å­©å­åœ¨å­¦ä¹ ä¸­æ”¶è·è‡ªä¿¡å’Œæˆé•¿ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬ï¼Œä¸€èµ·è§è¯æ›´å¤šå­©å­çš„è¿›æ­¥ï¼ 

""",
            
            "ğŸ‘¥ Task 2: æè¿°å›¾ç‰‡äººç‰©": "è¯·ä»”ç»†åˆ†æå›¾ç‰‡ä¸­çš„äººç‰©åŠåœºæ™¯ï¼Œå¹¶ç»“åˆçŸ¥è¯†åº“ä¸­çš„äº§å“ä¿¡æ¯ï¼Œåˆ›ä½œä¸€ç¯‡çº¦200å­—çš„æè¿°ï¼š...",
            "ğŸ” Task 3: æè¿°è§†è§’": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„æ‹æ‘„è§†è§’å’Œè§’åº¦ï¼ŒåŒ…æ‹¬ç›¸æœºçš„ä½ç½®ã€æ‹æ‘„é«˜åº¦å’Œç„¦è·ç­‰ä¿¡æ¯ï¼Œä»¥åŠå®ƒä»¬å¯¹æ•´ä½“ç”»é¢çš„å½±å“ã€‚",
            "ğŸ”¤ Task 4: æè¿°æ–‡å­—": "è¯·æè¿°å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è§çš„æ ‡å¿—ã€è¯´æ˜ã€å¹¿å‘Šç‰Œç­‰å†…å®¹ï¼Œå¹¶è§£é‡Šå®ƒä»¬çš„å¯èƒ½æ„ä¹‰ã€‚"
        }

        selected_prompts = [prompts[selected_task]]

        if st.button("ğŸš€ å¼€å§‹åˆ›ä½œ"):
            if not uploaded_files or not selected_prompts:
                st.warning("âš ï¸ è¯·ä¸Šä¼ å›¾ç‰‡å¹¶é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼")
            else:
                with st.spinner("å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
                    temp_image_paths = []
                    for i, uploaded_file in enumerate(uploaded_files):
                        temp_image_path = f"temp_image_{i}.png"
                        with open(temp_image_path, "wb") as f:
                            f.write(uploaded_file.read())
                        temp_image_paths.append(temp_image_path)
                    
                    results = connect_glm4vplus_api(selected_prompts, temp_image_paths)
                    
                    result_index = 0
                    total_results = len(uploaded_files) * len(selected_prompts)
                    for i, uploaded_file in enumerate(uploaded_files):
                        if result_index < len(results):
                            st.subheader("ç”Ÿæˆç»“æœ")
                            st.text_area("", value=results[result_index], height=300, key=f"results_text_{i}")
                            result_index += 1

    elif function_choice == "çŸ¥è¯†åº“æ£€ç´¢":
        st.subheader("çŸ¥è¯†åº“æ£€ç´¢")
        query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
        if st.button("ğŸ” æœç´¢"):
            if query:
                with st.spinner("æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
                    response = retrieve_from_knowledge_base(query)
                    st.subheader("æ£€ç´¢ç»“æœ")
                    full_response = ""
                    response_container = st.empty()
                    for chunk in response:
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            response_container.text_area("", value=full_response, height=300)
            else:
                st.warning("è¯·è¾“å…¥é—®é¢˜åå†æœç´¢ã€‚")

    elif function_choice == "çŸ¥è¯†åº“ç®¡ç†":
        st.subheader("çŸ¥è¯†åº“ç®¡ç†")
        
        st.subheader("ä¿®æ”¹çŸ¥è¯†åº“")
        new_name = st.text_input("æ–°çš„çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰ï¼š")
        new_description = st.text_area("æ–°çš„çŸ¥è¯†åº“æè¿°ï¼ˆå¯é€‰ï¼‰ï¼š")
        if st.button("ä¿®æ”¹çŸ¥è¯†åº“"):
            if new_name or new_description:
                result = modify_knowledge_base(new_name, new_description)
                st.json(result)
            else:
                st.warning("è¯·è‡³å°‘è¾“å…¥æ–°çš„åç§°æˆ–æè¿°ã€‚")
        
        st.subheader("æŸ¥è¯¢çŸ¥è¯†åº“")
        page = st.number_input("é¡µç ", min_value=1, value=1)
        size = st.number_input("æ¯é¡µæ•°é‡", min_value=1, max_value=100, value=10)
        if st.button("æŸ¥è¯¢çŸ¥è¯†åº“"):
            result = query_knowledge_base(page, size)
            st.json(result)

else:
    # User input and file upload for other assistants
    task_input = st.text_area("è¯·å‚è€ƒç¤ºä¾‹è¾“å…¥ï¼š", height=150)
    uploaded_files = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", type=["png", "jpg", "jpeg"], accept_multiple_files=True)
    submit_button = st.button("æäº¤ç”Ÿæˆ")

    # Initialize session state
    if 'history' not in st.session_state:
        st.session_state.history = []

    # Create an empty placeholder for dynamic updates
    response_placeholder = st.empty()

    # Status display and feedback
    if submit_button and selected_assistant and (task_input or uploaded_files):
        try:
            with st.spinner("æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚..."):
                # Prepare message content
                message_content = []
                if task_input:
                    message_content.append({
                        "type": "text",
                        "text": task_input
                    })
                
                for uploaded_file in uploaded_files:
                    # Upload image to OpenAI
                    file_response = client.files.create(
                        file=uploaded_file,
                        purpose="assistants"
                    )
                    
                    # Add image to message content
                    message_content.append({
                        "type": "image_file",
                        "image_file": {"file_id": file_response.id}
                    })

                # Create conversation thread
                thread = client.beta.threads.create(
                    messages=[
                        {
                            "role": "user",
                            "content": message_content,
                        }
                    ]
                )

                # Submit thread and wait for completion
                run = client.beta.threads.runs.create(
                    thread_id=thread.id,
                    assistant_id=selected_assistant['id']
                )

                # Wait for conversation to complete
                while run.status != "completed":
                    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
                    time.sleep(1)

                # Get conversation messages
                message_response = client.beta.threads.messages.list(thread_id=thread.id)
                messages = message_response.data
                latest_message = messages[0]
                
                # Display result using chat bubble style
                with response_placeholder.container():
                    st.chat_message("assistant").write(latest_message.content[0].text.value)

                # Update conversation history
                user_message = f"æ‚¨ï¼š{task_input}"
                if uploaded_files:
                    user_message += f" (ä¸Šä¼ äº† {len(uploaded_files)} å¼ å›¾ç‰‡)"
                st.session_state.history.append({"role": "user", "content": user_message})
                st.session_state.history.append({"role": "assistant", "name": selected_assistant['name'], "content": latest_message.content[0].text.value})

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
    else:
        if submit_button:
            if not selected_assistant:
                st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªåŠ©æ‰‹")
            elif not task_input and not uploaded_files:
                st.warning("è¯·è¾“å…¥é—®é¢˜æˆ–ä¸Šä¼ å›¾ç‰‡")

    # Display conversation history
    if st.session_state.history:
        st.subheader("å¯¹è¯å†å²")
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.history:
                if message["role"] == "user":
                    with st.chat_message("user"):
                        st.write(message["content"])
                else:
                    with st.chat_message("assistant", avatar=selected_assistant["icon"]):
                        st.write(f"{message['name']}: {message['content']}")

    # Use expander to collapse old conversations
    with st.expander("æŸ¥çœ‹å®Œæ•´å¯¹è¯å†å²"):
        for message in st.session_state.history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant", avatar=selected_assistant["icon"]):
                    st.write(f"{message['name']}: {message['content']}")
