import streamlit as st
import time
from openai import OpenAI
import random
from PIL import Image
import requests
import json
import base64
from zhipuai import ZhipuAI

# Initialize OpenAI client
# è¯»å–å¤šä¸ª API keys
api_keys = st.secrets["OPENAI_API_KEYS"]
# éšæœºé€‰æ‹©ä¸€ä¸ª API key
api_key = random.choice(api_keys)
# ä½¿ç”¨é€‰ä¸­çš„ API key åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key)

# æ™ºè°±AI API keys
zhipuai_api_keys = st.secrets["ZHIPU_API_KEYS"]

# å…è®¸è®¿é—®çš„å¯†ç åˆ—è¡¨
valid_passwords = [
    "Snaily!Yes",
    "PeteStory2024",
    "1"
]

# çŸ¥è¯†åº“IDï¼ˆå›ºå®šï¼‰
knowledge_id = "1842887159632314368"

# Define available Assistants
assistants = [
    {
        "name": "æè´æ‹‰", "id": "asst_SeUjvYOqni9yksDSC3EoqkQf", "alias": "ç»˜æœ¬æ¨èåŠ©æ‰‹", "icon": "ğŸ‘§ğŸ»",
        "description": "æˆ‘å¯ä»¥ä¸ºæ‚¨æ¨èé€‚åˆä¸åŒå¹´é¾„æ®µçš„ç²¾å½©ç»˜æœ¬ï¼Œæ¿€å‘å­©å­çš„é˜…è¯»å…´è¶£ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šç»™æˆ‘å®å®å°åï¼Œæ€§åˆ«ç”·å®è¿˜æ˜¯å¥³å®ï¼Œä»¥åŠå‡ºç”Ÿå¹´æœˆæ—¥å³å¯ã€‚ã€Œ èœ—èœ— å¥³å® 20191121 ã€ æˆ‘ä¼šå¸®ä½ å®šåˆ¶æ¨èåŒ¹é…ç»˜æœ¬çš„ã€‚"
    },
    {
        "name": "å¼ è‰¾ç±³", "id": "asst_IUatrIPuQQcfXW35sKW3wtk2", "alias": "äº²å­è‹±è¯­åŠ©æ‰‹", "icon": "ğŸ™‹â€â™€ï¸",
        "description": "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è®¾è®¡æœ‰è¶£çš„è‹±è¯­å­¦ä¹ æ´»åŠ¨ï¼Œè®©å­©å­è½»æ¾æŒæ¡è‹±è¯­ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šè¯·è®¾è®¡ä¸€ä¸ªé€‚åˆ7å²å­©å­çš„è‹±è¯­å•è¯æ¸¸æˆï¼Œä¸»é¢˜æ˜¯åŠ¨ç‰©ã€‚"
    },
    {
        "name": "èµµçš®ç‰¹", "id": "asst_8oQvvCwqc5bjeF3GOzEiWQP5", "alias": "çš®ç‰¹çŒ«æ–‡æ¡ˆåŠ©æ‰‹", "icon": "ğŸ±",
        "description": "æˆ‘å¯ä»¥å¸®æ‚¨åˆ›ä½œæœ‰è¶£çš„çš®ç‰¹çŒ«ç²¾è¯»è¥çš„æœ‹å‹åœˆæ–‡æ¡ˆï¼Œå†™å¾—å¿«åˆå†™å¾—å¥½ï¼Œå–µï¼ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šè¯·ä¸ºä¸€æœ¬æ–°çš„çš®ç‰¹çŒ«ç»˜æœ¬åˆ›ä½œä¸€ä¸ªå¸å¼•äººçš„ç®€çŸ­ä»‹ç»ï¼Œä¸»é¢˜æ˜¯çš®ç‰¹çŒ«å­¦ä¹ æ¸¸æ³³ã€‚"
    },
    {
        "name": "V3.5Mini", "id": "glm-4v-plus", "alias": "æˆªå›¾åˆ›ä½œæœ‹å‹åœˆåŠ©æ‰‹", "icon": "ğŸ¤–",
        "description": "æˆ‘æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½AIåŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œå›¾åƒåˆ†æã€çŸ¥è¯†åº“æ£€ç´¢å’Œç®¡ç†ç­‰ä»»åŠ¡ã€‚",
        "guidance": "ç¤ºä¾‹ï¼šä¸Šä¼ ä¸€å¼ å›¾ç‰‡å¹¶é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼Œå¦‚æè¿°å›¾ç‰‡å†…å®¹æˆ–åˆ†æäººç‰©ã€‚"
    }
]

# Streamlit app configuration
st.set_page_config(
    page_title="èœ—ç‰›æ™ºèƒ½",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for styling
st.markdown("""
<style>
    [data-testid="stSidebarContent"] {
        background-color: #FBF7FF !important;
    }
    .stRadio > label {
        background-color: rgba(255, 225, 225, 0.5);
        border-radius: 10px;
        padding: 10px;
        margin-bottom: 10px;
        cursor: pointer;
        transition: all 0.3s;
    }
    .stRadio > label:hover {
        background-color: rgba(255, 225, 225, 0.4);
    }
    .stRadio > label > div:first-child {
        display: none;
    }
    .assistant-icon {
        font-size: 2em;
        margin-right: 10px;
    }
    .assistant-name {
        font-weight: bold;
        margin-bottom: 5px;
    }
    .assistant-description {
        font-size: 0.9em;
        color: #555;
    }
    .guidance-example {
        background-color: rgba(237, 241, 255, 0.7);
        border-radius: 5px;
        padding: 10px;
        margin-top: 10px;
        font-size: 0.9em;
    }
    .assistant-title {
        font-size: 24px;
        font-weight: bold;
        margin-bottom: 20px;
    }
    .logo-title {
        display: flex;
        align-items: center;
        margin-bottom: 20px;
    }
    .logo-title h1 {
        font-size: 24px;
        margin: 0;
        margin-left: 10px;
    }
    .sidebar h1 {
        font-size: 1.2rem;
        margin-bottom: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# V3.5Mini functions
def connect_glm4vplus_api(prompts, images):
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    headers = {
        "Authorization": f"Bearer {random.choice(zhipuai_api_keys)}",
        "Content-Type": "application/json"
    }
    
    images_base64 = []
    for image in images:
        with open(image, "rb") as image_file:
            images_base64.append(base64.b64encode(image_file.read()).decode('utf-8'))
    
    messages = []
    for image_base64 in images_base64:
        for prompt in prompts:
            messages.append({
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_base64
                        }
                    },
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            })
    
    data = {
        "model": "glm-4v-plus",
        "messages": messages,
        "temperature": 0.8,
        "max_tokens": 1024,
        "stream": False,
        "tools": [
            {
                "type": "retrieval",
                "retrieval": {
                    "knowledge_id": knowledge_id,
                    "prompt_template": """
                    ä»æ–‡æ¡£
                    {{knowledge}}
                    ä¸­æ‰¾åˆ°ä¸å›¾ç‰‡ç›¸å…³çš„äº§å“ä¿¡æ¯ã€‚
                    """
                }
            }
        ]
    }

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        response_data = response.json()
        choices = response_data.get("choices", [])
        if not choices:
            return ["æ²¡æœ‰è¿”å›ç»“æœ"] * len(messages)
        return [choice.get("message", {}).get("content", "æ²¡æœ‰è¿”å›ç»“æœ") for choice in choices]
    else:
        return [f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, ä¿¡æ¯: {response.text}"] * len(messages)

def retrieve_from_knowledge_base(query):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    response = client.chat.completions.create(
        model="glm-4",
        messages=[
            {"role": "user", "content": query}
        ],
        tools=[
            {
                "type": "retrieval",
                "retrieval": {
                    "knowledge_id": knowledge_id,
                    "prompt_template": """
                    ä»æ–‡æ¡£
                    {{knowledge}}
                    ä¸­æ‰¾åˆ°é—®é¢˜
                    {{question}}
                    çš„ç­”æ¡ˆã€‚æ‰¾åˆ°åç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä½¿ç”¨æ¨¡å‹è‡ªèº«çŸ¥è¯†ï¼Œå¹¶å‘ŠçŸ¥ä¿¡æ¯ä¸æ¥è‡ªæ–‡æ¡£ã€‚
                    """
                }
            }
        ],
        stream=True
    )
    return response

def modify_knowledge_base(name, description):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    result = client.knowledge.modify(
        knowledge_id=knowledge_id,
        embedding_id=3,
        name=name,
        description=description
    )
    return result

def query_knowledge_base(page=1, size=10):
    client = ZhipuAI(api_key=random.choice(zhipuai_api_keys))
    result = client.knowledge.query(
        page=page,
        size=size
    )
    return result

# Sidebar for logo, title, and assistant selection
with st.sidebar:
    # Logo and title
    col1, col2 = st.columns([1, 3])
    with col1:
        try:
            logo = Image.open("./static/logo.png")
            st.image(logo, width=50)
        except FileNotFoundError:
            st.error("Logo file not found. Please check the file path.")
    with col2:
        st.markdown('<h1 class="logo-title">èœ—ç‰›æ™ºèƒ½</h1>', unsafe_allow_html=True)
    
    st.markdown('<h3>é€‰æ‹©ä½ çš„AIåŠ©æ‰‹</h3>', unsafe_allow_html=True)
    
    selected_assistant = st.radio(
        "é€‰æ‹©ä¸€ä¸ªåŠ©æ‰‹",
        options=assistants,
        format_func=lambda x: x['name'],
        label_visibility="collapsed"
    )
    
    # Display selected assistant information
    st.markdown(f"""
    <div style="background-color: rgba(255, 225, 225, 0.4); border-radius: 10px; padding: 10px; margin-top: 10px;">
        <span class="assistant-icon">{selected_assistant['icon']}</span>
        <div class="assistant-name">{selected_assistant['name']} ({selected_assistant['alias']})</div>
        <div class="assistant-description">{selected_assistant['description']}</div>
    </div>
    """, unsafe_allow_html=True)
    
    # Display guidance example
    st.markdown(f'<div class="guidance-example"><strong>æŒ‡å¯¼ç¤ºä¾‹ï¼š</strong><br>{selected_assistant["guidance"]}</div>', unsafe_allow_html=True)

# Main content area
st.markdown(f'<h1 class="assistant-title">{selected_assistant["icon"]} {selected_assistant["name"]}</h1>', unsafe_allow_html=True)

# Password verification for V3.5Mini
if selected_assistant["name"] == "V3.5Mini":
    password = st.text_input("è¯·è¾“å…¥å†…éƒ¨å¯†ç ï¼š", type="password")
    if password not in valid_passwords:
        st.warning("ğŸ”’ è¾“å…¥æœ‰æ•ˆå¯†ç ä»¥è®¿é—®V3.5Mini AIåº”ç”¨ã€‚")
        st.stop()

# V3.5Mini specific UI
if selected_assistant["name"] == "V3.5Mini":
    function_choice = st.radio("é€‰æ‹©åŠŸèƒ½", ["å›¾åƒåˆ†æ", "çŸ¥è¯†åº“æ£€ç´¢", "çŸ¥è¯†åº“ç®¡ç†"])

    if function_choice == "å›¾åƒåˆ†æ":
        st.subheader("ä¸Šä¼ å›¾ç‰‡")
        uploaded_files = st.file_uploader("è¯·é€‰æ‹©è¦ä¸Šä¼ çš„å›¾ç‰‡(æœ€å¤š5å¼ ,å•å¼ æ•ˆæœæœ€å¥½)ï¼š", type=["jpg", "jpeg", "png"], accept_multiple_files=True, key="fileUploader", help="æœ€å¤šå¯ä»¥ä¸Šä¼  5 å¼ å›¾ç‰‡ã€‚")

        if uploaded_files and len(uploaded_files) > 5:
            st.warning("âš ï¸ æœ€å¤šåªèƒ½ä¸Šä¼  5 å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
            uploaded_files = uploaded_files[:5]

        st.subheader("é€‰æ‹©ä»»åŠ¡")
        selected_task = st.radio("è¯·é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼š", [
            "ğŸ–¼ï¸ Task 1: æè¿°å›¾ç‰‡",
            "ğŸ‘¥ Task 2: æè¿°å›¾ç‰‡äººç‰©",
            "ğŸ” Task 3: æè¿°è§†è§’",
            "ğŸ”¤ Task 4: æè¿°æ–‡å­—"
        ])

        prompts = {
            "ğŸ–¼ï¸ Task 1: æè¿°å›¾ç‰‡": "è¯·ä»”ç»†åˆ†ææˆ‘ä¸Šä¼ çš„å›¾ç‰‡ï¼ˆåŒ…æ‹¬èŠå¤©æˆªå›¾ï¼Œå­¦å‘˜ç•™è¨€ï¼Œè¯„ä»·ï¼Œåé¦ˆï¼Œæˆé•¿æ—¶åˆ»ç­‰ï¼‰ï¼Œæå–å…³é”®ä¿¡æ¯å’Œæƒ…æ„Ÿå…ƒç´ ã€‚ç„¶åï¼Œè¯·åˆ›ä½œä¸€ç¯‡çº¦300å­—çš„æ–‡æ¡ˆå†…å®¹ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š...",
            "ğŸ‘¥ Task 2: æè¿°å›¾ç‰‡äººç‰©": "è¯·ä»”ç»†åˆ†æå›¾ç‰‡ä¸­çš„äººç‰©åŠåœºæ™¯ï¼Œå¹¶ç»“åˆçŸ¥è¯†åº“ä¸­çš„äº§å“ä¿¡æ¯ï¼Œåˆ›ä½œä¸€ç¯‡çº¦200å­—çš„æè¿°ï¼š...",
            "ğŸ” Task 3: æè¿°è§†è§’": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„æ‹æ‘„è§†è§’å’Œè§’åº¦ï¼ŒåŒ…æ‹¬ç›¸æœºçš„ä½ç½®ã€æ‹æ‘„é«˜åº¦å’Œç„¦è·ç­‰ä¿¡æ¯ï¼Œä»¥åŠå®ƒä»¬å¯¹æ•´ä½“ç”»é¢çš„å½±å“ã€‚",
            "ğŸ”¤ Task 4: æè¿°æ–‡å­—": "è¯·æè¿°å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è§çš„æ ‡å¿—ã€è¯´æ˜ã€å¹¿å‘Šç‰Œç­‰å†…å®¹ï¼Œå¹¶è§£é‡Šå®ƒä»¬çš„å¯èƒ½æ„ä¹‰ã€‚"
        }

        selected_prompts = [prompts[selected_task]]

        if st.button("ğŸš€ å¼€å§‹åˆ›ä½œ"):
            if not uploaded_files or not selected_prompts:
                st.warning("âš ï¸ è¯·ä¸Šä¼ å›¾ç‰‡å¹¶é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼")
            else:
                with st.spinner("å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
                    temp_image_paths = []
                    for i, uploaded_file in enumerate(uploaded_files):
                        temp_image_path = f"temp_image_{i}.png"
                        with open(temp_image_path, "wb") as f:
                            f.write(uploaded_file.read())
                        temp_image_paths.append(temp_image_path)
                    
                    results = connect_glm4vplus_api(selected_prompts, temp_image_paths)
                    
                    result_index = 0
                    total_results = len(uploaded_files) * len(selected_prompts)
                    for i, uploaded_file in enumerate(uploaded_files):
                        if result_index < len(results):
                            st.subheader("ç”Ÿæˆç»“æœ")
                            st.text_area("", value=results[result_index], height=300, key=f"results_text_{i}")
                            result_index += 1

    elif function_choice == "çŸ¥è¯†åº“æ£€ç´¢":
        st.subheader("çŸ¥è¯†åº“æ£€ç´¢")
        query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
        if st.button("ğŸ” æœç´¢"):
            if query:
                with st.spinner("æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
                    response = retrieve_from_knowledge_base(query)
                    st.subheader("æ£€ç´¢ç»“æœ")
                    full_response = ""
                    response_container = st.empty()
                    for chunk in response:
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            response_container.text_area("", value=full_response, height=300)
            else:
                st.warning("è¯·è¾“å…¥é—®é¢˜åå†æœç´¢ã€‚")

    elif function_choice == "çŸ¥è¯†åº“ç®¡ç†":
        st.subheader("çŸ¥è¯†åº“ç®¡ç†")
        
        st.subheader("ä¿®æ”¹çŸ¥è¯†åº“")
        new_name = st.text_input("æ–°çš„çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰ï¼š")
        new_description = st.text_area("æ–°çš„çŸ¥è¯†åº“æè¿°ï¼ˆå¯é€‰ï¼‰ï¼š")
        if st.button("ä¿®æ”¹çŸ¥è¯†åº“"):
            if new_name or new_description:
                result = modify_knowledge_base(new_name, new_description)
                st.json(result)
            else:
                st.warning("è¯·è‡³å°‘è¾“å…¥æ–°çš„åç§°æˆ–æè¿°ã€‚")
        
        st.subheader("æŸ¥è¯¢çŸ¥è¯†åº“")
        page = st.number_input("é¡µç ", min_value=1, value=1)
        size = st.number_input("æ¯é¡µæ•°é‡", min_value=1, max_value=100, value=10)
        if st.button("æŸ¥è¯¢çŸ¥è¯†åº“"):
            result = query_knowledge_base(page, size)
            st.json(result)

else:
    # User input and file upload for other assistants
    task_input = st.text_area("è¯·å‚è€ƒç¤ºä¾‹è¾“å…¥ï¼š", height=150)
    uploaded_files = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", type=["png", "jpg", "jpeg"], accept_multiple_files=True)
    submit_button = st.button("æäº¤ç”Ÿæˆ")

    # Initialize session state
    if 'history' not in st.session_state:
        st.session_state.history = []

    # Create an empty placeholder for dynamic updates
    response_placeholder = st.empty()

    # Status display and feedback
    if submit_button and selected_assistant and (task_input or uploaded_files):
        try:
            with st.spinner("æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚..."):
                # Prepare message content
                message_content = []
                if task_input:
                    message_content.append({
                        "type": "text",
                        "text": task_input
                    })
                
                for uploaded_file in uploaded_files:
                    # Upload image to OpenAI
                    file_response = client.files.create(
                        file=uploaded_file,
                        purpose="assistants"
                    )
                    
                    # Add image to message content
                    message_content.append({
                        "type": "image_file",
                        "image_file": {"file_id": file_response.id}
                    })

                # Create conversation thread
                thread = client.beta.threads.create(
                    messages=[
                        {
                            "role": "user",
                            "content": message_content,
                        }
                    ]
                )

                # Submit thread and wait for completion
                run = client.beta.threads.runs.create(
                    thread_id=thread.id,
                    assistant_id=selected_assistant['id']
                )

                # Wait for conversation to complete
                while run.status != "completed":
                    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
                    time.sleep(1)

                # Get conversation messages
                message_response = client.beta.threads.messages.list(thread_id=thread.id)
                messages = message_response.data
                latest_message = messages[0]
                
                # Display result using chat bubble style
                with response_placeholder.container():
                    st.chat_message("assistant").write(latest_message.content[0].text.value)

                # Update conversation history
                user_message = f"æ‚¨ï¼š{task_input}"
                if uploaded_files:
                    user_message += f" (ä¸Šä¼ äº† {len(uploaded_files)} å¼ å›¾ç‰‡)"
                st.session_state.history.append({"role": "user", "content": user_message})
                st.session_state.history.append({"role": "assistant", "name": selected_assistant['name'], "content": latest_message.content[0].text.value})

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
    else:
        if submit_button:
            if not selected_assistant:
                st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªåŠ©æ‰‹")
            elif not task_input and not uploaded_files:
                st.warning("è¯·è¾“å…¥é—®é¢˜æˆ–ä¸Šä¼ å›¾ç‰‡")

    # Display conversation history
    if st.session_state.history:
        st.subheader("å¯¹è¯å†å²")
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.history:
                if message["role"] == "user":
                    with st.chat_message("user"):
                        st.write(message["content"])
                else:
                    with st.chat_message("assistant", avatar=selected_assistant["icon"]):
                        st.write(f"{message['name']}: {message['content']}")

    # Use expander to collapse old conversations
    with st.expander("æŸ¥çœ‹å®Œæ•´å¯¹è¯å†å²"):
        for message in st.session_state.history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant", avatar=selected_assistant["icon"]):
                    st.write(f"{message['name']}: {message['content']}")
